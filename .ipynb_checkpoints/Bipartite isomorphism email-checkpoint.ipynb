{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums_idxs_dict(sum_vector):\n",
    "    '''Function that returns a dictionary of the sums values and their indexes in sum vector'''\n",
    "    return {k:list(np.where(sum_vector == k)[0]) for k in np.unique(sum_vector)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_sum_idxs_double_dict(arr1, arr2):\n",
    "    '''\n",
    "    Function that returns double dictionary of axis, sum value and indexes permutations from\n",
    "    two arrays. It finds all the possible permutations between the arrays based on every\n",
    "    value of the vector sum.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr1, arr2: 2D numpy arrays\n",
    "        Equal dimension arrays to be compared and to extract the permuted indexes.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    s_ip: dictionary\n",
    "        Dictionary of dictionary. First key is the axis. Second keys are sums values, i.e.\n",
    "        integers. Values are possible permutations (2D numpy arrays) in array notation,\n",
    "        therefore if the array has only one column, this column represent a new permutation\n",
    "        for the case of unique sum values, and this permutation should be now permanent.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> new_a = np.array([[0, 1, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0],\n",
    "                          [1, 1, 0, 0, 0],\n",
    "                          [0, 1, 1, 0, 1]])\n",
    "    >>> new_b = np.array([[1, 1, 0, 0, 1],\n",
    "                          [0, 1, 1, 1, 0],\n",
    "                          [0, 0, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0]])\n",
    "    >>> axis_sum_idxs_double_dict(new_a, new_b)\n",
    "    {0: {2: array([[0, 2, 3, 4],\n",
    "             [0, 1, 2, 4]]), 3: array([[1],\n",
    "             [3]])}, 1: {2: array([[2],\n",
    "             [2]]), 3: array([[0, 1, 3],\n",
    "             [0, 1, 3]])}}\n",
    "    \n",
    "    To access for example the Cauchy array created by uncertainty in axis 0, this means columns\n",
    "    uncertainty, and created also by uncertainty in all the columns adding up to 2, we need:\n",
    "    \n",
    "    >>> axis_sum_idxs_double_dict(new_a, new_b)[0][2]\n",
    "    array([[0, 2, 3, 4],\n",
    "           [0, 1, 2, 4]])\n",
    "    '''\n",
    "    \n",
    "    def sums_idxspermutations_dict(arr1, arr2, ax):\n",
    "        '''Function that finds all the possible permutations between the arrays based on vector\n",
    "        sum values.'''\n",
    "        i = 0\n",
    "        keys1_list = list(sums_idxs_dict(np.sum(arr1, ax)).keys())\n",
    "        keys2_list = list(sums_idxs_dict(np.sum(arr2, ax)).keys())\n",
    "        assert keys1_list == keys2_list\n",
    "        s_ip = {}\n",
    "        sidsa1a_values = sums_idxs_dict(np.sum(arr1, ax)).values()\n",
    "        sidsa2a_values = sums_idxs_dict(np.sum(arr2, ax)).values()\n",
    "        for v, w in zip(sidsa1a_values, sidsa2a_values):\n",
    "            assert len(v) == len(w)\n",
    "            if ax == 0:\n",
    "                array_value = np.concatenate((np.array([v]), np.array([w])))\n",
    "            elif ax == 1:\n",
    "                array_value = np.concatenate((np.array([w]), np.array([v])))\n",
    "            s_ip[keys1_list[i]] = array_value\n",
    "            i += 1\n",
    "        return s_ip\n",
    "    \n",
    "    dict_of_dict = {}\n",
    "    for j in (0, 1):\n",
    "        dict_of_dict[j] = sums_idxspermutations_dict(arr1, arr2, ax=j)\n",
    "    return dict_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_degeneracies_unique_only(arr1, arr2, ax, unique_sum_value, prints=False):\n",
    "    '''\n",
    "    Function that finds the double degeneracy for unique values in two permuted matrices and\n",
    "    their associated possible permutations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr1, arr2: 2D numpy arrays\n",
    "        Equal dimension arrays to be analyzed through their permuted indexes.\n",
    "    ax: int\n",
    "        Axis where the unique values vector is present.\n",
    "    unique_sum_value: int\n",
    "        Unique sum value to which its associated cauchy column must be used for locating all\n",
    "        possible permutations for every matrix element type in the perpendicular dimensions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    two_arrays_dict: dict\n",
    "        Dictionary of two 2D numpy arrays, one for each type of matrix element possible\n",
    "        permutations. First array for matrix element and assigned key 0, second array for\n",
    "        matrix element and assigned key 1. They are cauchy arrays for all the possible\n",
    "        permutations in the perpendicular dimension.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> new_a = np.array([[0, 1, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0],\n",
    "                          [1, 1, 0, 0, 0],\n",
    "                          [0, 1, 1, 0, 1]])\n",
    "    >>> new_b = np.array([[1, 1, 0, 0, 1],\n",
    "                          [0, 1, 1, 1, 0],\n",
    "                          [0, 0, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0]])\n",
    "                          \n",
    "    The unique column adding up to 3, with original column index 1 and permuted column index 3,\n",
    "    won't produce degeneracy in the rows for matrix element 0, which has original row index 1\n",
    "    and permuted row index 0. But his column will produce a triple degeneracy in the rows for\n",
    "    matrix element 1:\n",
    "                          \n",
    "    >>> obtain_degeneracies_unique_only(new_a, new_b, 0, 3)\n",
    "    {0: array([[0],\n",
    "            [1]]), 1: array([[1, 2, 3],\n",
    "            [0, 2, 3]])}\n",
    "            \n",
    "    Similarly, the unique row adding up to 2, will produce the following degeneracies in the\n",
    "    columns, where it is important to notice the difference in original and permuted\n",
    "    convention for rows or columns Cauchy arrays:\n",
    "            \n",
    "    >>> obtain_degeneracies_unique_only(new_a, new_b, 1, 2)\n",
    "    {0: array([[2, 3, 4],\n",
    "            [0, 1, 2]]), 1: array([[0, 1],\n",
    "            [3, 4]])}\n",
    "    '''\n",
    "\n",
    "    dict_of_dicts = axis_sum_idxs_double_dict(arr1, arr2)\n",
    "    cauchy_column = dict_of_dicts[ax][unique_sum_value]\n",
    "    if prints:\n",
    "        print(\"cauchy_column:\")\n",
    "        print(cauchy_column)\n",
    "    assert cauchy_column.shape == (2, 1)\n",
    "\n",
    "    arrs = [arr1, arr2]\n",
    "    zeros_idxs= []\n",
    "    ones_idxs= []\n",
    "    for a in (0, 1):\n",
    "        if ax == 0:\n",
    "            unique_vector = arrs[a][:, cauchy_column[a][0]]\n",
    "        elif ax == 1:\n",
    "            unique_vector = arrs[a][cauchy_column[1-a][0]]\n",
    "        zeros_idxs.append(np.where(unique_vector==0)[0])\n",
    "        ones_idxs.append(np.where(unique_vector==1)[0])\n",
    "        if prints:\n",
    "            print(\"\\na\", a)\n",
    "            print(\"cauchy_column[1-a][0]\", cauchy_column[1-a][0])\n",
    "            print(\"unique_vector\", unique_vector)\n",
    "            print(\"np.where(unique_vector==0)[0]\", np.where(unique_vector==0)[0])\n",
    "            print(\"zeros_idxs\", zeros_idxs)\n",
    "            print(\"np.where(unique_vector==1)[0]\", np.where(unique_vector==1)[0])\n",
    "            print(\"ones_idxs\", ones_idxs)\n",
    "    \n",
    "    two_arrays_dict = {}\n",
    "    two_arrays_dict[0] = np.concatenate(([zeros_idxs[1-ax]], [zeros_idxs[ax]]))\n",
    "    two_arrays_dict[1] = np.concatenate(([ones_idxs[1-ax]], [ones_idxs[ax]]))\n",
    "    return two_arrays_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cauchy_column:\n",
      "[[2]\n",
      " [2]]\n",
      "\n",
      "a 0\n",
      "cauchy_column[1-a][0] 2\n",
      "unique_vector [1 1 0 0 0]\n",
      "np.where(unique_vector==0)[0] [2 3 4]\n",
      "zeros_idxs [array([2, 3, 4])]\n",
      "np.where(unique_vector==1)[0] [0 1]\n",
      "ones_idxs [array([0, 1])]\n",
      "\n",
      "a 1\n",
      "cauchy_column[1-a][0] 2\n",
      "unique_vector [0 0 0 1 1]\n",
      "np.where(unique_vector==0)[0] [0 1 2]\n",
      "zeros_idxs [array([2, 3, 4]), array([0, 1, 2])]\n",
      "np.where(unique_vector==1)[0] [3 4]\n",
      "ones_idxs [array([0, 1]), array([3, 4])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: array([[2, 3, 4],\n",
       "        [0, 1, 2]]), 1: array([[0, 1],\n",
       "        [3, 4]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a = np.array([[0, 1, 0, 1, 1],\n",
    "                  [1, 0, 1, 1, 0],\n",
    "                  [1, 1, 0, 0, 0],\n",
    "                  [0, 1, 1, 0, 1]])\n",
    "new_b = np.array([[1, 1, 0, 0, 1],\n",
    "                  [0, 1, 1, 1, 0],\n",
    "                  [0, 0, 0, 1, 1],\n",
    "                  [1, 0, 1, 1, 0]])\n",
    "obtain_degeneracies_unique_only(new_a, new_b, 1, 2, prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythonic(s):\n",
    "    '''partition_intersections(x, y) is almost equivalent to list(pythonic(x + y).values()), \n",
    "    but lists order is different'''\n",
    "    all_elements = defaultdict(list)\n",
    "    for i, ss in enumerate(s):\n",
    "        for elem in ss:\n",
    "            all_elements[elem].append(i)\n",
    "    reversed = defaultdict(list)\n",
    "    for k, v in all_elements.items():\n",
    "        reversed[frozenset(v)].append(k)\n",
    "    return reversed#.keys()#.values()#list(reversed.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_intersections(x, y):\n",
    "    '''\n",
    "    Function that finds all Cartesian product intersections between two partitions x and y\n",
    "    of the same set. For example, partition_intersections(x, y) returns roughly the same as:\n",
    "    [np.intersect1d(a, b) for a in x for b in y]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: lists\n",
    "        Lists of lists, or list of 1D numpy arrays. Flattened lists of x and y should be equal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_lists: list\n",
    "        List of lists with all Cartesian product intersections between x and y.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> u = [[1, 2], [3, 4, 5], [6, 7, 8, 9, 10]]\n",
    "    >>> v = [[1, 3, 6, 7], [2, 4, 5, 8, 9, 10]]\n",
    "    >>> partition_intersections(u, v)\n",
    "    [[1], [2], [3], [4, 5], [6, 7], [8, 9, 10]]\n",
    "    '''\n",
    "    flattened_x = np.sort(np.concatenate((x), axis=None))\n",
    "    flattened_y = np.sort(np.concatenate((y), axis=None))\n",
    "    assert (flattened_x == flattened_y).all()\n",
    "    all_s = []\n",
    "    for sx in x:\n",
    "        for sy in y:\n",
    "            ss = list(filter(lambda i:i in sx, sy))\n",
    "            if ss:\n",
    "                all_s.append(ss)#np.array(ss))\n",
    "    return all_s\n",
    "    '''all_lists = list(pythonic(x + y).values()) #this is almost equivalent, but lists order is different\n",
    "    return all_lists'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullaxis_uniqueonly_degeneracies_intersection(fullaxis_dict, uniqueonly_dict):\n",
    "    '''\n",
    "    Function that applies partition_intersections among rows or cauchy arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fullaxis_dict: dict\n",
    "        Dictionary of all sums and their indexes degeneracy for a given axis, obtained from\n",
    "        axis_sum_idxs_double_dict.\n",
    "    uniqueonly_dict: dict\n",
    "        Dictionary of both degeneracies for given unique sum in axis perpendicular to the\n",
    "        previous one, obtained from obtain_degeneracies_unique_only.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    final_list: list\n",
    "        List of Cauchy arrays with partially complete degeneracy for given axis.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> new_a = np.array([[0, 1, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0],\n",
    "                          [1, 1, 0, 0, 0],\n",
    "                          [0, 1, 1, 0, 1]])\n",
    "    >>> new_b = np.array([[1, 1, 0, 0, 1],\n",
    "                          [0, 1, 1, 1, 0],\n",
    "                          [0, 0, 0, 1, 1],\n",
    "                          [1, 0, 1, 1, 0]])\n",
    "                          \n",
    "    If we want to find intersection of degeneracies due to all columns (axis 0), knowing the\n",
    "    degeneracies obtained due to rows (axis 1) with unique sum value 2, the respective Cauchy\n",
    "    arrays for columns are:\n",
    "    \n",
    "    >>> asiddnanb0 = axis_sum_idxs_double_dict(new_a, new_b)[0]\n",
    "    >>> oduonanb12 = obtain_degeneracies_unique_only(new_a, new_b, 1, 2)\n",
    "    >>> fullaxis_uniqueonly_degeneracies_intersection(asiddnanb0, oduonanb12)\n",
    "    [array([[2, 3, 4],\n",
    "            [0, 1, 2]]), array([[0],\n",
    "            [4]]), array([[1],\n",
    "            [3]])]\n",
    "    '''\n",
    "    \n",
    "    def dic_row(dic, row):\n",
    "        '''Function that contructs list of 1D numpy arrays (rows) from row of dictionary of\n",
    "        2D numpy arrays'''\n",
    "        return [arr[row] for arr in dic.values()]\n",
    "    \n",
    "    pdp0 = partition_intersections(dic_row(fullaxis_dict, 0), dic_row(uniqueonly_dict, 0))\n",
    "    pdp1 = partition_intersections(dic_row(fullaxis_dict, 1), dic_row(uniqueonly_dict, 1))\n",
    "    #print(pdp0)\n",
    "    #print(pdp1)\n",
    "    final_list = []\n",
    "    for m, n in zip(pdp0, pdp1):\n",
    "        final_list.append(np.concatenate(([m], [n])))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2, 3, 4],\n",
       "        [0, 1, 2]]), array([[0],\n",
       "        [4]]), array([[1],\n",
       "        [3]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a = np.array([[0, 1, 0, 1, 1],\n",
    "                  [1, 0, 1, 1, 0],\n",
    "                  [1, 1, 0, 0, 0],\n",
    "                  [0, 1, 1, 0, 1]])\n",
    "new_b = np.array([[1, 1, 0, 0, 1],\n",
    "                  [0, 1, 1, 1, 0],\n",
    "                  [0, 0, 0, 1, 1],\n",
    "                  [1, 0, 1, 1, 0]])\n",
    "asiddnanb0 = axis_sum_idxs_double_dict(new_a, new_b)[0]\n",
    "oduonanb12 = obtain_degeneracies_unique_only(new_a, new_b, 1, 2)\n",
    "fullaxis_uniqueonly_degeneracies_intersection(asiddnanb0, oduonanb12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_sum_values(sum_vector):\n",
    "    '''Function that returns the unique sum values for a given (row or column) sum vector'''\n",
    "    sid = sums_idxs_dict(sum_vector)\n",
    "    return [k for k, v in sid.items() if len(v)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def FINAL(a, b, prints=False, latex_prints=False):\n",
    "    partial_permutations_dict = {}\n",
    "    for j in (1, 0):\n",
    "        us_a = unique_sum_values(np.sum(a, axis=j))\n",
    "        us_b = unique_sum_values(np.sum(b, axis=j))\n",
    "        if latex_prints:\n",
    "            unique_a_idxs = [np.where(np.sum(a, axis=j)==k)[0][0]+1 for k in us_a]\n",
    "            unique_b_idxs = [np.where(np.sum(b, axis=j)==k)[0][0]+1 for k in us_b]\n",
    "            if j == 1:\n",
    "                print(\"**Unique rows sum values:\", us_a, \"have latex indexes:\")\n",
    "                print(np.concatenate(([unique_b_idxs], [unique_a_idxs])))\n",
    "            elif j == 0:\n",
    "                print(\"**Unique columns sum values:\", us_a, \"have latex indexes:\")\n",
    "                print(np.concatenate(([unique_a_idxs], [unique_b_idxs])))\n",
    "        if prints:\n",
    "            print(\"\\nj\", j)\n",
    "            print(np.sum(a, axis=j), \"us_a:\", us_a)\n",
    "            print(np.sum(b, axis=j), \"us_b:\", us_b)\n",
    "        assert us_a == us_b\n",
    "        my_dict = {}\n",
    "        c = 0\n",
    "        p_p = []\n",
    "        asiddabj = axis_sum_idxs_double_dict(a, b)[1-j]\n",
    "        if latex_prints:\n",
    "            if j == 1:\n",
    "                print(\"In the perpendicular dimension of the columns, the non unique columns sum values have degeneracy:\")\n",
    "            elif j == 0:\n",
    "                print(\"In the perpendicular dimension of the rows, the non unique rows sum values have degeneracy:\")\n",
    "            print(asiddabj)\n",
    "        for u in us_a:\n",
    "            #asiddabj = axis_sum_idxs_double_dict(a, b)[1-j]\n",
    "            oduoabju = obtain_degeneracies_unique_only(a, b, j, u)\n",
    "            fa_uo_di = fullaxis_uniqueonly_degeneracies_intersection(asiddabj, oduoabju)\n",
    "            if latex_prints:\n",
    "                print(\"-For unique value\", u, \"the perpendicular degeneracies of its zeros and ones is:\", oduoabju, \"thus the final intersection of it with its perpendicular full axis degeneracy is:\")\n",
    "                print(fa_uo_di)\n",
    "            if prints:\n",
    "                print(\"u\", u)\n",
    "                print(\"fa_uo_di:\")\n",
    "                print(fa_uo_di)\n",
    "            my_dict[u] = fa_uo_di\n",
    "            c += 1\n",
    "            if c == 1:\n",
    "                p_p = my_dict[us_a[c-1]]\n",
    "            if c > 1:\n",
    "                t_dict = {}\n",
    "                for i in (0, 1):\n",
    "                    rows_minus1 = [sublist[i] for sublist in my_dict[us_a[c-1]]]\n",
    "                    rows_partial = [sublist[i] for sublist in p_p]\n",
    "                    t_dict[i] = partition_intersections(rows_minus1, rows_partial)\n",
    "                p_p = [np.concatenate(([m], [n])) for m, n in zip(t_dict[0], t_dict[1])]\n",
    "            if prints:\n",
    "                print(\"p_p\", p_p)\n",
    "        if p_p:\n",
    "            partial_permutations_dict[1-j] = p_p\n",
    "        else:\n",
    "            asiddabj = axis_sum_idxs_double_dict(a, b)[1-j]\n",
    "            partial_permutations_dict[1-j] = [v for v in asiddabj.values()]\n",
    "    \n",
    "    if latex_prints:\n",
    "        print(\"Finally, all the partial permutations due to unique values are:\", partial_permutations_dict)\n",
    "\n",
    "    return partial_permutations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_nonready_arrays(new_a, new_b, ab_dict, prints=False, prints_counter=False):\n",
    "    ready_arrays_dict = {}\n",
    "    nonready_arrays_dict = {}\n",
    "    for k, v in ab_dict.items():\n",
    "        nonready_arrays = []\n",
    "        if prints:\n",
    "            print(\"\\nk\", k)\n",
    "            print(\"v\", v)\n",
    "        r = n = 0\n",
    "        for w in v:\n",
    "            if w.shape[1] == 1:\n",
    "                r += 1\n",
    "                if prints:\n",
    "                    print(r, \"READY:\")\n",
    "                    print(w)\n",
    "                if r == 1:\n",
    "                    ready_array = w\n",
    "                    if prints:\n",
    "                        print(\"ready_array:\")\n",
    "                        print(ready_array)\n",
    "                else:\n",
    "                    ready_array = np.concatenate((ready_array, w), axis=1)\n",
    "                    if prints:\n",
    "                        print(\"ready_array:\")\n",
    "                        print(ready_array)\n",
    "            else:\n",
    "                n += 1\n",
    "                if prints:\n",
    "                    print(n, \"not ready:\")\n",
    "                    print(w)\n",
    "                    print(\"nonready_arrays\", nonready_arrays)\n",
    "                nonready_arrays.append(w)\n",
    "        if prints_counter:\n",
    "            print(\"Final counters (r: ready, n: not ready)\")\n",
    "            print(\"r:\", r)\n",
    "            print(\"n:\", n)\n",
    "        if r != 0:\n",
    "            ready_arrays_dict[k] = ready_array\n",
    "        if n != 0:\n",
    "            nonready_arrays_dict[k] = nonready_arrays\n",
    "    return ready_arrays_dict, nonready_arrays_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permuted_order_cauchy_to_python(arr, ax):\n",
    "    '''\n",
    "    Example\n",
    "    -------\n",
    "    >>> cauchy = np.array([[1, 0, 2, 3, 5],\n",
    "                           [0, 2, 3, 1, 5]]) \n",
    "    >>> poctpC = permuted_order_cauchy_to_python(cauchy, 1)\n",
    "    >>> poctpC    \n",
    "    array([2, 0, 3, 1, 5])\n",
    "    >>> poctpR = permuted_order_cauchy_to_python(cauchy, 0)\n",
    "    >>> poctpR\n",
    "    array([1, 3, 0, 2, 5])\n",
    "    '''\n",
    "    cauchy_order = arr[:, np.argsort(arr[1-ax])]\n",
    "    python_permutation_order = cauchy_order[ax]    \n",
    "    return python_permutation_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITERATIVE FUNCTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dicts(to_update_ready_dict, to_update_nonready_dict, AA, BB, idx0=0, idx1=0, prints=False):\n",
    "    \n",
    "    tup_nr = (to_update_nonready_dict[0][idx0], to_update_nonready_dict[1][idx1])\n",
    "\n",
    "    A = AA[tup_nr[1][1]][:, tup_nr[0][0]]\n",
    "    B = BB[tup_nr[1][0]][:, tup_nr[0][1]]\n",
    "    r_n_tuple = ready_nonready_arrays(A, B, FINAL(A, B))\n",
    "    ready_AB_dict = r_n_tuple[0]\n",
    "    nonready_AB_dict = r_n_tuple[1]\n",
    "    ready_sub_dict = {k: tup_nr[k][np.arange(2)[:, None], ready_AB_dict[k]] for k in \n",
    "                      ready_AB_dict.keys()}\n",
    "    nonready_sub_dict = {k: [tup_nr[k][np.arange(2)[:, None], v] for v in \n",
    "                             nonready_AB_dict[k]] for k in nonready_AB_dict.keys()}\n",
    "    if prints:\n",
    "        print(\"\\n++++++++++++update_dicts++++++++++++\")\n",
    "        print(\"tup_nr\", tup_nr)\n",
    "        print(\"A:\")\n",
    "        print(A)\n",
    "        print(\"B:\")\n",
    "        print(B)\n",
    "        print(\"ready_sub_dict\", ready_sub_dict)\n",
    "        print(\"nonready_sub_dict\", nonready_sub_dict)\n",
    "    for ax in ready_sub_dict:\n",
    "        if prints:\n",
    "            print(\"AXIS ax\", ax, \"analysis for ready_sub_dict:\")\n",
    "            print(\"initial to_update_ready_dict[ax]\", to_update_ready_dict[ax])\n",
    "            print(\"ready_sub_dict[ax]\", ready_sub_dict[ax])\n",
    "        for col in ready_sub_dict[ax].T:\n",
    "            cauchy_col = np.reshape(col, (2, 1))\n",
    "            if not (to_update_ready_dict[ax] == cauchy_col).all(axis=0).any():\n",
    "                to_update_ready_dict[ax] = np.concatenate((to_update_ready_dict[ax],\n",
    "                                                           cauchy_col), axis=1)\n",
    "                if prints:\n",
    "                    print(\"to_update_ready_dict[ax]:\")\n",
    "                    print(to_update_ready_dict[ax])\n",
    "                    print(\"initial to_update_nonready_dict[ax]\", to_update_nonready_dict[ax])\n",
    "                nr0_bool = [cauchy_col[0] in arr[0] for arr in to_update_nonready_dict[ax]]\n",
    "                nr1_bool = [cauchy_col[1] in arr[1] for arr in to_update_nonready_dict[ax]]\n",
    "                assert nr0_bool.index(True) == nr1_bool.index(True)\n",
    "                true_arr_idx = nr0_bool.index(True)\n",
    "                arr_to_update = to_update_nonready_dict[ax][true_arr_idx]\n",
    "                #print(\"initial arr_to_update\", arr_to_update)\n",
    "                idxs = [(i, np.where(arr_to_update[i]==cauchy_col[i])[0][0]) for i in (0, 1)]\n",
    "                #print(\"idxs\", idxs)\n",
    "                atus0 = arr_to_update.shape[0]\n",
    "                atus1 = arr_to_update.shape[1]\n",
    "                idxs = [i*atus1+j for i, j in idxs]\n",
    "                #print(\"idxs\", idxs)\n",
    "                arr_to_update = np.delete(arr_to_update, idxs).reshape(atus0, atus1-1)\n",
    "                #print(\"final arr_to_update\", arr_to_update)\n",
    "                if arr_to_update.shape == (2, 1):\n",
    "                    to_update_ready_dict[ax] = np.concatenate((to_update_ready_dict[ax],\n",
    "                                                               arr_to_update), axis=1)\n",
    "                    to_update_nonready_dict[ax].pop(true_arr_idx)\n",
    "                    if prints:\n",
    "                        print(\"if, to_update_ready_dict instead:\")\n",
    "                        print(\"to_update_ready_dict[ax]\", to_update_ready_dict[ax])\n",
    "                        print(\"to_update_nonready_dict[ax]\", to_update_nonready_dict[ax])\n",
    "                else:\n",
    "                    to_update_nonready_dict[ax][true_arr_idx] = arr_to_update\n",
    "                    if prints:\n",
    "                        print(\"else, to_update_nonready_dict[ax] now:\")\n",
    "                        print(\"to_update_nonready_dict[ax]\", to_update_nonready_dict[ax])\n",
    "    for ax in nonready_sub_dict:\n",
    "        if prints:\n",
    "            print(\"AXIS ax\", ax, \"analysis for nonready_sub_dict:\")\n",
    "            print(\"initial to_update_nonready_dict[ax]\", to_update_nonready_dict[ax])\n",
    "            print(\"nonready_sub_dict[ax]\", nonready_sub_dict[ax])\n",
    "        nrdax0 = [a[0] for a in to_update_nonready_dict[ax]]\n",
    "        nrsdax0 = [a[0] for a in nonready_sub_dict[ax]]\n",
    "        nrdax1 = [a[1] for a in to_update_nonready_dict[ax]]\n",
    "        nrsdax1 = [a[1] for a in nonready_sub_dict[ax]]\n",
    "        PDP0 = [v for v in pythonic(nrdax0 + nrsdax0).values()]\n",
    "        PDP1 = [pythonic(nrdax1 + nrsdax1)[k] for k in pythonic(nrdax0 + nrsdax0).keys()]\n",
    "        new_list = []\n",
    "        for m, n in zip(PDP0, PDP1):\n",
    "            new_list.append(np.concatenate(([m], [n])))\n",
    "        idx_to_pop = []\n",
    "        for c, arr in enumerate(new_list):\n",
    "            if ((arr.shape == (2, 1)) and\n",
    "                not (to_update_ready_dict[ax] == arr).all(axis=0).any()):\n",
    "                to_update_ready_dict[ax] = np.concatenate((uto_update_ready_dict[ax], arr),\n",
    "                                                          axis=1)\n",
    "                if prints:\n",
    "                    print(\"To update in to_update_ready_dict[ax] the arr:\", arr)\n",
    "                    print(\"to_update_ready_dict[ax]:\")\n",
    "                    print(to_update_ready_dict[ax])\n",
    "                idx_to_pop.append(c)\n",
    "            elif (arr.shape == (2, 1)) and (to_update_ready_dict[ax] == arr).all(axis=0).any():\n",
    "                if prints:\n",
    "                    print(\"Already updated in to_update_ready_dict[ax] the arr:\", arr)\n",
    "                idx_to_pop.append(c)\n",
    "        for i, idx in enumerate(idx_to_pop):\n",
    "            new_list.pop(idx - i) #because when poping an index, it changes the future indexes\n",
    "        to_update_nonready_dict[ax] = new_list\n",
    "        if prints:\n",
    "            print(\"to_update_nonready_dict[ax]\", to_update_nonready_dict[ax])\n",
    "\n",
    "    print(\"++++++++++++update_dicts++++++++++++\\n\")\n",
    "    return to_update_ready_dict, to_update_nonready_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 3, 5]\n",
    "b = a\n",
    "a[:] = [x+2 for x in a]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL pairs [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4)]\n",
      "0 20 (0, 0)\n",
      "1 20 (0, 1)\n",
      "2 20 (0, 2)\n",
      "3 20 (0, 3)\n",
      "4 20 (0, 4)\n",
      "5 20 (1, 0)\n",
      "6 20 (1, 1)\n",
      "7 20 (1, 2)\n",
      "8 20 (1, 3)\n",
      "9 20 (1, 4)\n",
      "10 20 (2, 0)\n",
      "11 20 (2, 1)\n",
      "12 20 (2, 2)\n",
      "13 20 (2, 3)\n",
      "If 3 in position 1 dissapears, then:\n",
      "partial [(0, 0), (0, 1), (0, 2), (0, 4), (1, 0), (1, 1), (1, 2), (1, 4), (2, 0), (2, 1), (2, 2), (2, 4), (3, 0), (3, 1), (3, 2), (3, 4)]\n",
      "partial_pairs [(0, 0), (0, 1), (0, 2), (0, 4), (1, 0), (1, 1), (1, 2), (1, 4), (2, 0), (2, 1), (2, 2)]\n",
      "14 16 (3, 2)\n",
      "15 16 (3, 4)\n"
     ]
    }
   ],
   "source": [
    "def explore_pairs(r0, r1, chg0, chg1, change='both', dissapears=False):\n",
    "    assert chg0<=r0\n",
    "    assert chg1<=r1\n",
    "    pairs = []\n",
    "    for pair in it.product(range(r0), range(r1)):\n",
    "        pairs.append(pair)\n",
    "        #\n",
    "    print(\"ORIGINAL pairs\", pairs)\n",
    "    #for pair in pairs:\n",
    "        #\n",
    "    i = 0\n",
    "    while i < len(pairs):\n",
    "        pair = pairs[i]\n",
    "        \n",
    "        print(i, len(pairs), pair)\n",
    "        if (pair[0]==chg0) and (pair[1]==chg1):\n",
    "            if change=='first':\n",
    "                if dissapears:\n",
    "                    print(\"If {} in position 0 dissapears, then:\".format(chg0))\n",
    "                    partial = [p for p in pairs if p[0]!=chg0]\n",
    "                    print(\"partial\", partial)\n",
    "                    partial_pairs = [p for p in partial if p in pairs[:i]]\n",
    "                    print(\"partial_pairs\", partial_pairs)\n",
    "                    pairs = partial\n",
    "                    #print([p if p[0]<chg0 else (p[0]-1, p[1]) for p in partial])\n",
    "                    #continue\n",
    "                else:\n",
    "                    print(\"If {} in position 0 changes, then restarts:\".format(chg0))\n",
    "                    for p in pairs:\n",
    "                        if (p[0]==chg0) and (p[1]<chg1):\n",
    "                            print(p)\n",
    "                    print(\":end\")\n",
    "            elif change=='second':\n",
    "                if dissapears:\n",
    "                    print(\"If {} in position 1 dissapears, then:\".format(chg1))\n",
    "                    partial = [p for p in pairs if p[1]!=chg1]\n",
    "                    print(\"partial\", partial)\n",
    "                    partial_pairs = [p for p in partial if p in pairs[:i]]\n",
    "                    print(\"partial_pairs\", partial_pairs)\n",
    "                    pairs = partial\n",
    "                else:\n",
    "                    print(\"Or if {} in position 1 changes, then restarts:\".format(chg1))\n",
    "                    for p in pairs:\n",
    "                        if (p[0]<chg0) and (p[1]==chg1):\n",
    "                            print(p)\n",
    "                    print(\":end\")\n",
    "            elif change=='both':\n",
    "                print(\"Or if both position changes, then restarts:\")\n",
    "                for p in pairs:\n",
    "                    if ((p[0]==chg0) and (p[1]<chg1)) or ((p[0]<chg0) and (p[1]==chg1)):\n",
    "                        print(p)\n",
    "                print(\":end\")\n",
    "                \n",
    "        i += 1\n",
    "        \n",
    "explore_pairs(4, 5, 2, 3, change='second', dissapears=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apparently the best idea is to append every tuple to a list of used ones, and then check, for every tuple in the iteratively updated nonready dictionary, if the tuple is already appended or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-18-4b121aeaf88a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-4b121aeaf88a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    already_checked_tups_nr = []\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    while to_update_nonready_dict[0] and to_update_nonready_dict[1]:\n",
    "\n",
    "    already_checked_tups_nr = []\n",
    "    tup_nr = (to_update_nonready_dict[0][pair[0]], to_update_nonready_dict[1][pair[1]])\n",
    "\n",
    "        \n",
    "        breaker = False\n",
    "        for p, pair in enumerate(pairs):\n",
    "            print(p, pair)\n",
    "            \n",
    "            if latex_prints:\n",
    "                print(\"tup_nr\", tup_nr)\n",
    "                print(\"A:\")\n",
    "                print(A)\n",
    "                print(\"B:\")\n",
    "                print(B)\n",
    "                print(\"Updated dicts:\")\n",
    "                print(\"ready_sub_dict\", ready_sub_dict)\n",
    "                print(\"nonready_sub_dict\", nonready_sub_dict)\n",
    "\n",
    "            print(\"tup_nr BEFORE IF\", tup_nr)\n",
    "            \n",
    "            if not ( any((tup_nr[0] == x[0]).all() for x in already_checked_tups_nr) and\n",
    "                    any((tup_nr[1] == x[1]).all() for x in already_checked_tups_nr) ):\n",
    "                \n",
    "            already_checked_tups_nr.append(tup_nr)\n",
    "            print(\"already_checked_tups_nr\", already_checked_tups_nr)\n",
    "                \n",
    "            else:\n",
    "                print(\"ELSE\")\n",
    "                print(\"tup_nr\", tup_nr)\n",
    "                print(\"already_checked_tups_nr\", already_checked_tups_nr)\n",
    "                print(\"tup_nr already in already_checked_tups_nr, so moving forward\")\n",
    "                \n",
    "        if (np.array_equal(initial_ready_dict[0], to_update_ready_dict[0]) and\n",
    "            np.array_equal(initial_ready_dict[1], to_update_ready_dict[1])):\n",
    "            print(\"There was no change in the last iteration therefore IT SHOULD GO TO NEXT PAIR:\", pairs[p+1])\n",
    "            if p+1 == len(pairs):\n",
    "                print(\"p+1 == len(pairs), this is the last one, IMPLIES breaker = True\")\n",
    "                breaker = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blablabla(A, B, prints=False, latex_prints=True):\n",
    "    ready_nonready_tuple = ready_nonready_arrays(A, B, FINAL(A, B))\n",
    "    ready_dict = ready_nonready_tuple[0]\n",
    "    nonready_dict = ready_nonready_tuple[1]\n",
    "    pairs = []\n",
    "    for pair in it.product(range(len(nonready_dict[0])), range(len(nonready_dict[1]))):\n",
    "        pairs.append(pair)\n",
    "    print(\"pairs\", pairs)\n",
    "    if prints:\n",
    "        print(\"*********blablabla*********\")\n",
    "        print(\"ready_dict\", ready_dict)\n",
    "        print(\"nonready_dict\", nonready_dict)\n",
    "        print(\"pairs\", pairs)\n",
    "        \n",
    "    if (0 in nonready_dict) and (1 in nonready_dict):\n",
    "        SsS = 0\n",
    "        while (nonready_dict[0]) and (nonready_dict[1]):\n",
    "            if latex_prints:\n",
    "                print(\"\\n\")\n",
    "                print(\"SsS\", SsS)\n",
    "                print(\"pairs[SsS]\", pairs[SsS])\n",
    "                print(\"ready_dict\", ready_dict)\n",
    "                print(\"nonready_dict\", nonready_dict)\n",
    "            #ready_dict, nonready_dict, br = update_dicts(ready_dict, nonready_dict, A, B,\n",
    "            #                                             prints=prints)\n",
    "            i0 = 0\n",
    "            i1 = 0\n",
    "            if latex_prints:\n",
    "                tup_nr = (nonready_dict[0][i0], nonready_dict[1][i1]) #optional, only to print\n",
    "                print(\"tup_nr\", tup_nr)\n",
    "            ready_dict, nonready_dict = update_dicts(ready_dict, nonready_dict, A, B, i0, i1,\n",
    "                                                         prints=prints)\n",
    "            SsS += 1\n",
    "            if latex_prints:\n",
    "                print(\"AND THE\", SsS, \"th ready_dict OBTAINED IS\", ready_dict)\n",
    "                print(\"AND THE\", SsS, \"th nonready_dict OBTAINED IS\", nonready_dict)\n",
    "                print(\"*********blablabla*********\")\n",
    "            if False:#br == True:\n",
    "                print(\"BREAK triggered\")\n",
    "                break\n",
    "\n",
    "    return ready_dict, nonready_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_indexes_permutations(arr):\n",
    "    '''\n",
    "    Example\n",
    "    -------\n",
    "    >>> all_indexes_permutations(np.reshape(np.arange(6), (2,3)))\n",
    "    array([[[0, 1, 2],\n",
    "            [3, 4, 5]],\n",
    "\n",
    "           [[0, 1, 2],\n",
    "            [3, 5, 4]],\n",
    "\n",
    "           [[0, 1, 2],\n",
    "            [4, 3, 5]],\n",
    "\n",
    "           [[0, 1, 2],\n",
    "            [4, 5, 3]],\n",
    "\n",
    "           [[0, 1, 2],\n",
    "            [5, 3, 4]],\n",
    "\n",
    "           [[0, 1, 2],\n",
    "            [5, 4, 3]]])\n",
    "    '''\n",
    "    assert arr.shape[0] == 2\n",
    "    cases_number = np.math.factorial(arr.shape[1])\n",
    "    multi_array = np.zeros((cases_number, arr.shape[0], arr.shape[1]), dtype=int)\n",
    "    j = 0\n",
    "    for i in it.permutations(arr[1]):\n",
    "        new_array = np.stack((arr[0], i))\n",
    "        multi_array[j] = new_array\n",
    "        j += 1\n",
    "    return multi_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "## Input dimensions of random matrix to create (rows, cols): dim\n",
    "## Input maximum number of brute force permutations to try: max_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (17, 11)\n",
    "max_perm = 200000 # less than 5 minutes in my laptop, try 50000 for fast results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The (random) input array a and its random permutation b (if you get an error, restart from here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(2, size=dim)\n",
    "ar = np.arange(a.shape[0])\n",
    "np.random.shuffle(ar)\n",
    "ac = np.arange(a.shape[1])\n",
    "np.random.shuffle(ac)\n",
    "b = a[ar][:,ac]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOW we execute the full algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "urd, unrd = blablabla(a, b)#, prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the row and colum indexes where we still have a degeneracy, a brute force approach is implemented, where every row in the cauchy array contributes with the factorial of its number of columns to the total, and it is calculated beforehand in brute_force_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dims = [arr.shape[1] for arr in [a for sublist in unrd.values() for a in sublist]]\n",
    "col_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brute_force_permutations = np.prod(col_dims)\n",
    "brute_force_permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when implementing the brute force permutations of the remaining degenerate indexes, the (unique) counter index where the permutation between a and b is found is called successful_permutation_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if brute_force_permutations < max_perm:\n",
    "\n",
    "    aip0 = [all_indexes_permutations(arr) for arr in unrd[0]]\n",
    "    aip1 = [all_indexes_permutations(arr) for arr in unrd[1]]\n",
    "\n",
    "    c = 0\n",
    "    for j in it.product(*aip0):\n",
    "        perm0 = np.concatenate(j, axis=-1)\n",
    "        #print(perm0)\n",
    "        funrd = {}\n",
    "        for i in it.product(*aip1):\n",
    "            c += 1\n",
    "            #print(\"c\", c)\n",
    "            perm1 = np.concatenate(i, axis=-1)\n",
    "            #print(perm1)\n",
    "            funrd[0] = perm0\n",
    "            funrd[1] = perm1\n",
    "            #print(\"funrd\", funrd)\n",
    "            d2 = [urd, funrd]\n",
    "            d = {}\n",
    "            for k in urd.keys():\n",
    "                if k in funrd.keys():\n",
    "                    d[k] = np.hstack([dd[k] for dd in d2])\n",
    "                else:\n",
    "                    d[k] = urd[k]\n",
    "            poctpd11 = permuted_order_cauchy_to_python(d[1], 1)\n",
    "            poctpd00 = permuted_order_cauchy_to_python(d[0], 0)\n",
    "            p_a = a[poctpd11][:, poctpd00]\n",
    "            if (p_a == b).all():\n",
    "                print(\"SUCCESS!!!\")\n",
    "                successful_permutation_counter = c\n",
    "                print(\"successful_permutation_counter:\", successful_permutation_counter)\n",
    "                rows_permutation = permuted_order_cauchy_to_python(d[1], 1)\n",
    "                cols_permutation = permuted_order_cauchy_to_python(d[0], 0)\n",
    "    print(\"Total number of performed brute force permutations:\", c)\n",
    "    \n",
    "else:\n",
    "    print(\"Restart the matrices, the number of permutations is too large:\", brute_force_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, the required permutations in python indexing style are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python to LaTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_networks = np.array([[[1,1,1],\n",
    "                          [1,1,0],\n",
    "                          [1,0,0],\n",
    "                          [1,0,0]],\n",
    "                         [[1,1,1],\n",
    "                          [1,1,0],\n",
    "                          [1,0,0],\n",
    "                          [0,1,0]],\n",
    "                         [[1,1,1],\n",
    "                          [1,1,0],\n",
    "                          [1,0,0],\n",
    "                          [0,0,1]],\n",
    "                         [[1,1,1],\n",
    "                          [1,1,0],\n",
    "                          [0,0,1],\n",
    "                          [0,0,1]]])\n",
    "full_networks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = string.ascii_uppercase[:full_networks.shape[0]]\n",
    "rows = string.ascii_lowercase[:full_networks.shape[1]]\n",
    "cols = string.ascii_lowercase[-full_networks.shape[2]:]\n",
    "id_list = [p[0]+p[1] for p in it.product(networks, rows+cols)]\n",
    "id_list\n",
    "#len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_x0 = 0\n",
    "left_y0 = 0\n",
    "delta_x = 1\n",
    "delta_y = -1\n",
    "left_x_locations = [left_x0 for i in range(len(rows))] + [left_x0+delta_x for i in range(len(cols))]\n",
    "left_y_locations = [left_y0+delta_y*i for i in range(len(rows))] + [left_y0+delta_y*i-1/2 for i in range(len(cols))]\n",
    "Delta_X = 3\n",
    "x_locations = [round(j+k*Delta_X, 5) for k in [0, 1.1, 2.3, 3.5] for j in left_x_locations]#[j+k*Delta_X for k in range(len(networks)) for j in left_x_locations]#uniform distance\n",
    "y_locations = left_y_locations*full_networks.shape[0]\n",
    "x_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_networks.reshape((-1,full_networks.shape[2]))\n",
    "np.sum(full_networks, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_networks.transpose(0,2,1)#.reshape(-1,full_networks.shape[1])\n",
    "np.sum(full_networks, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "positions = []\n",
    "colors_vertices = []\n",
    "colors_flat = ['teal', 'blue', 'violet', 'red']\n",
    "arrays_sums_list = [np.sum(full_networks, axis=2), np.sum(full_networks, axis=1)]\n",
    "shapes_product = full_networks.shape[0]*(full_networks.shape[1] + full_networks.shape[2])\n",
    "shapes_sum = full_networks.shape[1] + full_networks.shape[2]\n",
    "for i in range(shapes_product):\n",
    "    j = i-shapes_sum*(i//shapes_sum)\n",
    "    #print(i, i//shapes_sum, j)\n",
    "    if j < full_networks.shape[1]:\n",
    "        #print(\"first array\")\n",
    "        #print(0, i//shapes_sum, j)\n",
    "        labels.append(arrays_sums_list[0][i//shapes_sum, j])\n",
    "        positions.append('left')\n",
    "        colors_vertices.append(colors_flat[j])\n",
    "    else:\n",
    "        #print(\"second array\")\n",
    "        #print(1, i//shapes_sum, j-full_networks.shape[1])\n",
    "        labels.append(arrays_sums_list[1][i//shapes_sum, j-full_networks.shape[1]])\n",
    "        positions.append('right')\n",
    "        colors_vertices.append('black')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertices = pd.DataFrame({'id': id_list,\n",
    "                            'x': x_locations,\n",
    "                            'y': y_locations,\n",
    "                            'label': labels,\n",
    "                            'position': positions,\n",
    "                            'fontcolor': colors_vertices\n",
    "                  })\n",
    "df_vertices.to_csv(r'vertices.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nafn = np.argwhere(full_networks)#[:,[0,1,0,2]]\n",
    "nafn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_list = [networks[r[0]]+rows[r[1]] for r in nafn]\n",
    "u_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_list = [networks[r[0]]+cols[r[2]] for r in nafn]\n",
    "v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_edges = [colors_flat[i] for i in np.argwhere(full_networks)[:,1]]\n",
    "colors_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = pd.DataFrame({'u': u_list,\n",
    "                         'v': v_list,\n",
    "                         'color': colors_edges\n",
    "                        })\n",
    "df_edges.to_csv(r'edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_and_more_to_latex(arr):\n",
    "    big_arr = np.concatenate((np.sum(arr, axis=1).reshape((arr.shape[0],1)), arr), axis=1)\n",
    "    c_s = 'c'\n",
    "    print(\"$\\\\begin{blockarray}{\" + c_s*(big_arr.shape[1]) + \"}\")\n",
    "    print(\"\\\\begin{block}{c[\" + c_s*(big_arr.shape[1]-1) + \"]}\")\n",
    "    for r, row in enumerate(big_arr):\n",
    "        elel = ''\n",
    "        for e, el in enumerate(np.char.mod('%d', row)):\n",
    "            if e != big_arr.shape[1]-1:\n",
    "                elel += el + ' & '\n",
    "            else:\n",
    "                elel += el + ' \\\\'\n",
    "        if r == 0:\n",
    "            elel += 'topstrut \\\\\\\\'\n",
    "        elif r == big_arr.shape[0]-1:\n",
    "            elel += 'botstrut \\\\\\\\'\n",
    "        else:\n",
    "            elel += '\\\\'\n",
    "        print(elel)\n",
    "    print(\"\\\\end{block}\")\n",
    "    els = ''\n",
    "    for el in np.char.mod('%d', np.sum(arr, axis=0)):\n",
    "        els += ' & ' + el\n",
    "    print(els)\n",
    "    print(\"\\end{blockarray}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atest = np.array([[1, 0, 0, 1, 1, 1, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 0],\n",
    "       [1, 1, 0, 0, 1, 1, 1],\n",
    "       [0, 1, 1, 1, 0, 0, 0],\n",
    "       [1, 0, 1, 1, 1, 1, 1],\n",
    "       [0, 1, 0, 1, 1, 1, 1],\n",
    "       [1, 0, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 0, 1, 0, 1, 1],\n",
    "       [1, 0, 1, 0, 0, 0, 0],\n",
    "       [1, 0, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 1, 1, 1]])\n",
    "arrays_and_more_to_latex(atest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest = np.array([[1, 0, 0, 1, 0, 0, 1],\n",
    "       [0, 1, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 1, 1, 1],\n",
    "       [0, 0, 1, 1, 0, 0, 0],\n",
    "       [0, 1, 1, 1, 1, 1, 0],\n",
    "       [1, 1, 1, 1, 1, 1, 0],\n",
    "       [1, 1, 0, 0, 0, 1, 0],\n",
    "       [1, 1, 1, 0, 1, 0, 0],\n",
    "       [1, 1, 0, 1, 1, 0, 0],\n",
    "       [1, 0, 1, 1, 1, 0, 0]])\n",
    "arrays_and_more_to_latex(btest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivots(arr):\n",
    "    un_r, id_r, co_r = np.unique(np.sum(arr, axis=1), return_index=True, return_counts=True)\n",
    "    unique_in_sum_row_idxs = np.sort(id_r[np.where(co_r==1)[0]])\n",
    "    un_c, id_c, co_c = np.unique(np.sum(arr, axis=0), return_index=True, return_counts=True)\n",
    "    unique_in_sum_col_idxs = np.sort(id_c[np.where(co_c==1)[0]])\n",
    "    return unique_in_sum_row_idxs, unique_in_sum_col_idxs, arr[unique_in_sum_row_idxs, :][:, unique_in_sum_col_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uisri, uisci, subarr = pivots(atest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uisri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uisci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
